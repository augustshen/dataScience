# Pr((avg(X) - E(avg(X)))/SE(avg(X)) <= ((p + 2 * SE(avg(X))) - E(avg(X))) / SE(avg(X))) - Pr((avg(X) - E(avg(X)))/SE(avg(X)) <= ((p - 2 * SE(avg(X))) - E(avg(X))) / SE(avg(X)))
# Pr(Z <= 2 * SE(avg(X)) / SE(avg(X))) - Pr(Z <= -2 * SE(avg(X)) / SE(avg(X)))
# Pr(Z <= 2) - Pr(Z <= -2)
pnorm(2) - pnorm(-2)
# 2.3 A Monte Carlo Simulation for the CLT
B <- 10000
N <- 1000
X_hat <- replicate(B, {
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
mean(X)
})
# not work above because there is no p
p <- 0.45
N <- 1000
X <- sample(c(0, 1), size = N, relace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
# 2.3 A Monte Carlo Simulation for the CLT
p <- 0.45
B <- 10000
N <- 1000
X_hat <- replicate(B, {
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
mean(X)
})
mean(X_hat)
sd(X_hat)
library(gridExtra)
p2 <- data.frame(X_hat = X_hat) %>% ggplot(aes(sample = X_hat)) +
stat_qq(dparams = list(mean= mean(X_hat), sd=sd(X_hat))) +
geom_abline() +
ylab("X_hat") +
xlab("Theoretical normal")
library(gridExtra)
p1 <- data.frame((X_hat= X_hat)) %>% ggplot(aes(X_hat)) +
geom_histogram(binwidth = 0.005, color="black")
library(tidyverse)
library(gridExtra)
p1 <- data.frame((X_hat= X_hat)) %>% ggplot(aes(X_hat)) +
geom_histogram(binwidth = 0.005, color="black")
p2 <- data.frame(X_hat = X_hat) %>% ggplot(aes(sample = X_hat)) +
stat_qq(dparams = list(mean= mean(X_hat), sd=sd(X_hat))) +
geom_abline() +
ylab("X_hat") +
xlab("Theoretical normal")
grid.arrange(p1, p2, nrow = 1)
# 2.5 Bias: Why not run a very large Poll
N <- 100000
p <- seq(0.35, 0.65, length = 100)
SE <- sapply(p, function(x) 2 * sqrt(x * (1 - x) / N))
data.frame(p = p, SE = SE) %>% ggplot(aes(p, SE)) + geom_line()
N <- seq(100, 5000, len = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
hist(se)
plot(N, se)
?pnorm
# Confidence Intervals
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
ggplot(aes(year, temperature)) +
geom_point() +
geom_smooth() +
ggtitle("Average Yearly Temperatures in the New Haven")
# Confidence Intervals
library(tidyverse)
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
ggplot(aes(year, temperature)) +
geom_point() +
geom_smooth() +
ggtitle("Average Yearly Temperatures in the New Haven")
p <- 0.45
N <- 1000
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
c(X_hat -  2 * SE_hat, X_hat + 2 * SE_hat)
z <- qnorm(0.995)
z
pnorm(qnorm(0.995))
pnorm(1 - qnorm(0.995))
pnorm(z) - pnorm(-z)
qnorm(0.975)
# 3.2 A Monte Carlo Simulation for Confidence Intervals
B <- 10000
inside <- replicate(B, {
X <- sample(c(0, 1), size = N, replace = TRuE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
between(p, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat)
})
inside <- replicate(B, {
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
between(p, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat)
})
mean(inside)
(2 * X_hat - 1) + c(-2, 2) * 2 * sqrt(X_hat * (1 - X_hat) / sqrt(N))
# 3.4 power
N <- 25
X_hat <- 0.48
(2 * X_hat - 1) + c(-2, 2) * 2 * sqrt(X_hat * (1 - X_hat) / sqrt(N))
# 3.4 power
N <- 2500
X_hat <- 0.48
(2 * X_hat - 1) + c(-2, 2) * 2 * sqrt(X_hat * (1 - X_hat) / sqrt(N))
N <- 100
z <- sqrt(N) * 0.02 / 0.5
1 - (pnorm(z) - pnorm(-z))
qnorm(1)
qnorm(2)
qnorm(0.95)
qnorm(0.05)
# Confidence Intervals
library(tidyverse)
data("nhtemp")
# exercises
# ex1
# Load the data
data(polls_us_election_2016)
# exercises
# ex1
# Load the data
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)
# Confidence Intervals
library(tidyverse)
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
ggplot(aes(year, temperature)) +
geom_point() +
geom_smooth() +
ggtitle("Average Yearly Temperatures in the New Haven")
p <- 0.45
N <- 1000
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
c(X_hat -  2 * SE_hat, X_hat + 2 * SE_hat)
# Statistical Models
# 4.1 Poll Aggregators
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1) / 2
confidence_intervals <- sapply(Ns, function(N) {
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
2 * c(X_hat, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat) - 1
})
polls <- data.frame(poll=1:ncol(confidence_intervals),
t(confidence_intervals),
sample_size = Ns)
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls
sum(polls$sample_size)
d_hat <- polls %>% summarize(avg = sum(estimate * sample_size) / sum(sample_size)) %>%
.$avg
p_hat <- (1 + d_hat) / 2
moe <- 2 * 1.96 * sqrt(p_hat * (1 - p_hat) / sum(polls$sample_size))
moe
round(d_hat* 100, 1)
round(moe * 100, 1)
# 4.3 Poll Data and Pollster Bias
data("polls_us_election_2016")
names(polls_us_election_2016)
polls <- polls_us_election_2016 %>% filter(state == "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))
polls <- polls %>% mutate(spread = rawpoll_clinton/100 - rawpoll_trump / 100)
d_hat <- polls %>% summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%
.$d_hat
moe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))
moe
polls %>% ggplot(aes(spread)) + geom_histogram(color="black", binwidth = .01)
polls %>% group_by(pollster) %>% summarize(n())
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
ggplot(aes(pollster, spread)) +
geom_point() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
polls %>% group_by(pollster) %>% filter(n() >= 6) %>%
summarize(se = 2 * sqrt(p_hat * (1 - p_hat) / median(samplesize)))
# 4.4 Data-Driven Models
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>% ungroup()
one_poll_per_pollster %>% ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)
# in new model d and sigema are both unknown
sd(one_poll_per_pollster$spread)
results <- one_poll_per_pollster %>%
summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
mutate(start = avg - 1.96 * se, end = avg + 1.96 * se)
round(results * 100, 1)
# ex6
# Load the libraries and data you need for the following exercises
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")
# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>%
filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
enddate >= "2016-10-15" &
state == "U.S.") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
# Make a boxplot with points of the spread for each pollster
polls %>% ggplot(aes(pollster, spread)) + geom_boxplot() + geom_point()
# Bayesian Statistics
# hierarchical model
# 5.2 Bayes' Theorem
# Pr(A|B) = Pr(A and B) / Pr(B) = Pr(B | A) * Pr(A) / Pr(B)
prev <- 0.00025
N <- 100000
outcome <- sample(C("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1 - prev))
outcome <- sample(C("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1 - prev))
# Statistical Models
# 4.1 Poll Aggregators
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1) / 2
confidence_intervals <- sapply(Ns, function(N) {
X <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))
X_hat <- mean(X)
SE_hat <- sqrt(X_hat * (1 - X_hat) / N)
2 * c(X_hat, X_hat - 2 * SE_hat, X_hat + 2 * SE_hat) - 1
})
polls <- data.frame(poll=1:ncol(confidence_intervals),
t(confidence_intervals),
sample_size = Ns)
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls
# Bayesian Statistics
# hierarchical model
# 5.2 Bayes' Theorem
# Pr(A|B) = Pr(A and B) / Pr(B) = Pr(B | A) * Pr(A) / Pr(B)
prev <- 0.00025
N <- 100000
outcome <- sample(C("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1 - prev))
outcome <- sample(C(1, 0), N, replace = TRUE, prob = c(prev, 1 - prev))
outcome <- sample(c(0, 1), N, replace = TRUE, prob = c(prev, 1 - prev))
outcome <- sample(c("Disease", "Healthy"), N, replace = TRUE, prob = c(prev, 1 - prev))
N_D <- sum(outcome == "Disease")
N_D
test <- vector("character", N)
accuracy <- 0.99
N_H <- sum(outcome == "Healthy")
accuracy <- 0.99
test <- vector("character", N)
test[outcome == "Disease"] <- sample(c("+", "-"), N_D, replace = TRUE, prob = c(accuracy, 1 - accuracy))
test[outcome == "Healthy"] <- sample(c("-", "+"), N_H, replace = TRUE, prob = c(accuracy, 1 - accuracy))
table(outcome, test)
# Election Forecasting
# d~N(miu, tau) describes our best guess had we not seen any polling data
# X_hat|d~N(d, sigma) describes randomness due to sampling and the pollster effect
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B * mu + (1 - B) * Y
posterior_se <- sqrt(1/(1/sigma^2 + 1/tau^2))
posterior_mean
posterior_se
posterior_mean + c(-1.96, 1.96) * posterior_se
# 6.2 Mathematical Representations of Models
J <- 6
N <- 2000
d <- 0.021
p <- (d + 1) / 2
X <- d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
I <- 5
J <- 6
N <- 2000
X <- sapply(1:I, function(i) {
d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})
# h is the difference of the pollers
I <- 5
J <- 6
N <- 2000
d <- 0.021
p <- (d + 1) / 2
h <- rnorm(I, 0, 0.025)
X <- sapply(1:I, function(i) {
d + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + 0.025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau ^2)
posterior_mean <- B * mu + (1 - B) * Y
posterior_se <- sqrt(1/(1/sigma^2 + 1/tau^2))
1 - pnorm(0, posterior_mean, posterior_se)
# 6.3 Predicting the Electoral College
results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)
# 6.3 Predicting the Electoral College
library(tidyverse)
results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)
results <- polls_us_election_2016 %>%
filter(state != "U.S." & !grepl("CD", state) &
enddate >= "2016-10-31" &
(grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
mutate(spread= rawpoll_clinton/100 - rawpoll_trump/100) %>%
group_by(state) %>%
summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
mutate(state = as.character(state))
results %>% arrange(abs(avg))
results <- left_join(results, results_us_election_2016, by = "state")
results_us_election_2016 %>% filter(!state %in% results$state)
results <- results %>% mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
results %>% mutate(sigma = sd/sqrt(n),
B = sigma^2 / (sd^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2))) %>%
arrange((abs(posterior_mean)))
mean(clinton_EV>269)
clinton_EV <- replicate(1000, {
results %>% mutate(sigma = sd / sqrt(n),
B = sigma^2/(sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B)*avg,
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)),
simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
clinton = ifelse(simulated_result>0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton)) %>%
.$clinton + 7## 7 for Rhode Island and D.C.
})
mean(clinton_EV>269)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
mu <- 0
tau <- 0.02
clinton_EV <- replicate(1000, {
results %>% mutate(sigma = sd / sqrt(n),
B = sigma^2/(sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B)*avg,
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)),
simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
clinton = ifelse(simulated_result>0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton)) %>%
.$clinton + 7## 7 for Rhode Island and D.C.
})
mean(clinton_EV>269)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) + geom_histogram(binwidth = 1)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline((xintercept = 269))
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
results %>% mutate(sigma = sqrt(sd^2/n + bias_sd^2),
B = sigma^2/(sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)),
simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
clinton = ifelse(simulated_result>0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton) + 7) %>% .$clinton ## 7 for Rhode Island and D.C.
})
data.frame(clinton_EV_2) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
results %>% mutate(sigma = sqrt(sd^2/n + bias_sd^2),
B = sigma^2/(sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)),
simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
clinton = ifelse(simulated_result>0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton) + 7) %>% .$clinton ## 7 for Rhode Island and D.C.
})
data.frame(clinton_EV_2) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
data.frame(clinton_EV_2) %>%
ggplot(aes(clinton_EV_2)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
# 6.4 Forecasting
one_pollster <- polls_us_election_2016 %>%
filter(pollster == "Ipsos" & state == "U.S.") %>%
mutate(spread = rawpoll_clinton / 100 - rawpoll_trump/100)
se <- one_pollster %>%
summarize(empirical = sd(spread),
theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread))/min(samplesize)))
se
one_pollster %>% ggplot(aes(spread)) +
geom_histogram(binwidth = 0.01, color = "black")
polls_us_election_2016 %>%
filter(state == "U.S." & enddate >= "2016-07-01") %>%
group_by(pollster) %>%
filter(n() >= 10) %>%
ungroup() %>%
mutate(spread = rawpoll_clinton / 100 - rawpoll_trump / 100) %>%
ggplot(aes(enddate, spread)) + geom_smooth(method = "loess", span = 0.1) +
geom_point(aes(color=pollster), show.legend = FALSE, alpha = 0.6)
# consider the time of trend f(t)
polls_us_election_2016 %>%
filter(state == "U.S." & enddate >= "2016-07-01") %>%
select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
gather(candidate, percentage, -enddate, -pollster) %>%
mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
group_by(pollster) %>%
filter(n()>=10) %>%
ungroup() %>%
ggplot(aes(enddate, percentage, color = candidate)) +
geom_point(show.legend = FALSE, alpha = 0.4) +
geom_smooth(method = "loess", span = 0.15) +
scale_y_continuous(limits = c(30, 50))
# ex3
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
# exercises module 1
# ex1
# Load the libraries and data
library(dplyr)
library(dslabs)
# ex3
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
cis <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
# Create a table called `polls` that filters by  state, date, and reports the spread
polls <- polls_us_election_2016 %>%
filter(state != "U.S." & enddate >= "2016-10-31") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
# Create an object called `cis` that columns for the lower and upper confidence intervals. Select the columns indicated in the instructions.
cis <- polls %>% mutate(X_hat = (spread+1)/2, se = 2*sqrt(X_hat*(1-X_hat)/samplesize),
lower = spread - qnorm(0.975)*se, upper = spread + qnorm(0.975)*se) %>%
select(state, startdate, enddate, pollster, grade, spread, lower, upper)
# ex3
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
cis <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")
names(cis)
# Create an object called `p_hits` that summarizes the proportion of hits for each pollster that has more than 5 polls.
p_hits <- cis %>% mutate(hit = actual_spread >= lower & actual_spread <= upper) %>% group_by(pollster)
p_hits
p_hits <- cis %>% mutate(hit = actual_spread >= lower & actual_spread <= upper) %>% group_by(pollster) %>% filter(n() >= 5) %>% summarize(proportion_hits = mean(hit), n = n())
p_hits
p_hits <- cis %>% mutate(hit = actual_spread >= lower & actual_spread <= upper) %>% group_by(pollster) %>% filter(n() >= 5) %>% summarize(proportion_hits = mean(hit), n = n()) %>% arrange(desc(proportion_hits))
p_hits
p_hits <- cis %>% mutate(hit = actual_spread >= lower & actual_spread <= upper) %>% group_by(pollster) %>% filter(n() >= 5) #%>% summarize(proportion_hits = mean(hit), n = n()) %>% arrange(desc(proportion_hits))
p_hits
?reorder
# 6.5 The t-Distribution
# for values smaller than 30, we need to be cautious about using the CLT
# only if the data follow a normal distribution
# Z = (X_hat - d) / (sigma / sqrt(N))
# actually, we don't know sigma, so use this
# Z = (X_hat - d) / (s*sqrt(N))
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>% summarize(avg = mean(spread), moe = Z*sd(spread)/sqrt(length(spread))) %>% mutate(start = avg - moe, end = avg + moe)
# 6.3 Predicting the Electoral College
library(tidyverse)
one_poll_per_pollster %>% summarize(avg = mean(spread), moe = Z*sd(spread)/sqrt(length(spread))) %>% mutate(start = avg - moe, end = avg + moe)
# 6.5 The t-Distribution
# for values smaller than 30, we need to be cautious about using the CLT
# only if the data follow a normal distribution
# Z = (X_hat - d) / (sigma / sqrt(N))
# actually, we don't know sigma, so use this
# Z = (X_hat - d) / (s*sqrt(N))
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>% summarize(avg = mean(spread), moe = Z*sd(spread)/sqrt(length(spread))) %>% mutate(start = avg - moe, end = avg + moe)
one_poll_per_pollster %>% summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>% mutate(start = avg - moe, end = avg + moe)
qt(0.975, 14)
qnorm(0.975)
?between
??between
